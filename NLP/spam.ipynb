{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9128ecf3",
   "metadata": {},
   "source": [
    "## Importação de bibliotecas e leitura de DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c173ffa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mneighbors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from itertools import combinations \n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn import tree, svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# The file does not use commas as a separator, the \"sep\" argument helps by setting the tab character as the separator\n",
    "# The file does not have a header row, the \"names\" argument is used to define the header for each column\n",
    "df = pd.read_csv(\"smsspamcollection/SMSSpamCollection.csv\", sep=\"\\t\", names=[\"Type\", \"Text\"], header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a600479",
   "metadata": {},
   "source": [
    "## Análise do DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27730f2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Printa os primeiros elementos do Dataframe\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m.head())\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Da informações das colunas do Dataframe\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.info())\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the first elements of the DataFrame\n",
    "print(df.head())\n",
    "# Provide information about the DataFrame columns\n",
    "print(df.info())\n",
    "# Give a brief description of the DataFrame data\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Type\" column has values \"ham\" and \"spam\", so I changed it to \"categorical\"\n",
    "# More information about pandas \"categorical\" can be found at: https://pandas.pydata.org/docs/reference/api/pandas.Categorical.html\n",
    "df['Type'] = df['Type'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ceee47",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4f2a10",
   "metadata": {},
   "source": [
    "The formula for TF-IDF is:  \n",
    "\n",
    "(term frequency in the document) × log((total number of documents) / (number of documents containing the term)).\n",
    "\n",
    "I implemented the algorithm to test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ceb15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go', 15.53326712644149), ('until', 15.53326928005903), ('jurong', 15.53326479335059), ('point,', 15.53326479335059), ('crazy..', 15.53326479335059), ('Available', 15.533265152288006), ('only', 15.533290098122793), ('in', 15.533394720277643), ('bugis', 15.533265331756665), ('n', 15.533285252577684), ('great', 15.533277176616993), ('world', 15.53326766484631), ('la', 15.533264972819314), ('e', 15.533276279284001), ('buffet...', 15.53326479335059), ('Cine', 15.53326479335059), ('there', 15.533283816856105), ('got', 15.533299789142573), ('amore', 15.53326479335059), ('wat...', 15.533266588036382)]\n"
     ]
    }
   ],
   "source": [
    "# This can be simplified, but I did it just for testing (a class would also be good)\n",
    "tf_count = {}\n",
    "num_documents = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    line = row['Text'].strip().split()\n",
    "    num_documents += 1\n",
    "    seen = set()\n",
    "    for word in line:\n",
    "        if word not in seen:\n",
    "            if word in tf_count:\n",
    "                tf_count[word] += 1\n",
    "            else:\n",
    "                tf_count[word] = 1\n",
    "        seen.add(word)\n",
    "\n",
    "res = []\n",
    "for index, row in df.iterrows():\n",
    "    line = row['Text'].strip().split()\n",
    "    local_res = []\n",
    "    local_m = {}\n",
    "    for word in line:\n",
    "        if word in local_m:\n",
    "            local_m[word] += 1\n",
    "        else:\n",
    "            local_m[word] = 1\n",
    "    for word in line:\n",
    "        local_res.append((word, local_m[word] * math.log(num_documents/1e-3 + tf_count[word])))\n",
    "    res.append(local_res)\n",
    "\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e44a93",
   "metadata": {},
   "source": [
    "I’m going to end up using Scikit-learn’s TF-IDF implementation:  \n",
    "\n",
    "reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875964e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>ú1</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.009510</td>\n",
       "      <td>0.018111</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>0.006843</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.004299</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.004138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.240128</td>\n",
       "      <td>0.654425</td>\n",
       "      <td>0.233155</td>\n",
       "      <td>0.265767</td>\n",
       "      <td>0.250210</td>\n",
       "      <td>0.304792</td>\n",
       "      <td>0.290193</td>\n",
       "      <td>0.227745</td>\n",
       "      <td>0.510769</td>\n",
       "      <td>0.256937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273889</td>\n",
       "      <td>0.320935</td>\n",
       "      <td>0.296818</td>\n",
       "      <td>0.354245</td>\n",
       "      <td>0.290067</td>\n",
       "      <td>0.273938</td>\n",
       "      <td>0.160467</td>\n",
       "      <td>0.184833</td>\n",
       "      <td>0.224784</td>\n",
       "      <td>0.308912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 8713 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                00          000       000pes  008704050406         0089  \\\n",
       "count  5572.000000  5572.000000  5572.000000   5572.000000  5572.000000   \n",
       "mean      0.000402     0.001161     0.000042      0.000094     0.000045   \n",
       "std       0.009510     0.018111     0.003123      0.004943     0.003352   \n",
       "min       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "max       0.240128     0.654425     0.233155      0.265767     0.250210   \n",
       "\n",
       "              0121  01223585236  01223585334   0125698789           02  ...  \\\n",
       "count  5572.000000  5572.000000  5572.000000  5572.000000  5572.000000  ...   \n",
       "mean      0.000055     0.000052     0.000082     0.000092     0.000352  ...   \n",
       "std       0.004083     0.003888     0.004314     0.006843     0.009281  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "max       0.304792     0.290193     0.227745     0.510769     0.256937  ...   \n",
       "\n",
       "             zhong       zindgi          zoe    zogtorius         zoom  \\\n",
       "count  5572.000000  5572.000000  5572.000000  5572.000000  5572.000000   \n",
       "mean      0.000049     0.000058     0.000104     0.000064     0.000052   \n",
       "std       0.003669     0.004299     0.005467     0.004746     0.003886   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       0.273889     0.320935     0.296818     0.354245     0.290067   \n",
       "\n",
       "              zouk        zyada           èn           ú1          〨ud  \n",
       "count  5572.000000  5572.000000  5572.000000  5572.000000  5572.000000  \n",
       "mean      0.000049     0.000029     0.000033     0.000040     0.000055  \n",
       "std       0.003670     0.002150     0.002476     0.003011     0.004138  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       0.273938     0.160467     0.184833     0.224784     0.308912  \n",
       "\n",
       "[8 rows x 8713 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer   = TfidfVectorizer()\n",
    "tfidx_matrix = vectorizer.fit_transform(df['Text'])\n",
    "tfidx_df     = pd.DataFrame(tfidx_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidx_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04072f9b",
   "metadata": {},
   "source": [
    "## Dataset train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb34305",
   "metadata": {},
   "source": [
    "This section is easy using Scikit implementations, but later will get a bit harder with K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6616b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidx_df\n",
    "Y = df['Type']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64cc76",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21366f",
   "metadata": {},
   "source": [
    "### Decision Tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43590ac6",
   "metadata": {},
   "source": [
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "\n",
    "Statquest's video:\n",
    "https://www.youtube.com/watch?v=_L39rN6gz7Y&t=429s\n",
    "\n",
    "Scikit-Learn:  \n",
    "https://scikit-learn.org/stable/modules/tree.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a37e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nparameters = {\\n    'max_depth': [5, 10, 20, 50],\\n    'min_samples_split': [2, 5, 10],\\n    'min_samples_leaf': [1, 2, 4],\\n}\\n\\nmodel_dtree = GridSearchCV(model_dtree, parameters)\\nmodel_dtree = model_dtree.fit(x_train, y_train)\\n\\ny_pred = model_dtree.predict(x_test)\\naccuracy_score(y_pred, y_test)\\nf1_score(y_pred, y_test, average='macro')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dtree = tree.DecisionTreeClassifier()\n",
    "\n",
    "\"\"\"\n",
    "The next block of code was used to test GRID SEARCH with the Decision Tree, but it became very slow so I commented it out.  \n",
    "\n",
    "Grid Search is a method to find the best parameters for a Machine Learning model.  \n",
    "It does this by exhaustively testing all possible combinations of specified values until it finds the combination that yields the best performance.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    'max_depth': [5, 10, 20, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "model_dtree = GridSearchCV(model_dtree, parameters)\n",
    "model_dtree = model_dtree.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model_dtree.predict(x_test)\n",
    "accuracy_score(y_pred, y_test)\n",
    "f1_score(y_pred, y_test, average='macro')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f0052",
   "metadata": {},
   "source": [
    "### Support Vector Machines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22e6d7",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "It tries to find the best boundary known as hyperplane that separates different classes in the data.\n",
    "\n",
    "Statequest's videos:  \n",
    "https://www.youtube.com/watch?v=efR1C6CvhmE  \n",
    "https://www.youtube.com/watch?v=Toet3EiSFcM  \n",
    "https://www.youtube.com/watch?v=Qc5IyLW_hns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c33a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9820531227566404\n",
      "0.9598398437184672\n"
     ]
    }
   ],
   "source": [
    "model_svm = svm.SVC()\n",
    "model_svm.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model_svm.predict(x_test)\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "print(f1_score(y_pred, y_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede396ca",
   "metadata": {},
   "source": [
    "### K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2a451",
   "metadata": {},
   "source": [
    "The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as non-generalizing machine learning methods, since they simply “remember” all of its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree).\n",
    "\n",
    "\n",
    "Statquest's video:\n",
    "https://www.youtube.com/watch?v=HVXime0nQeI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abbe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6900691336368678"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors = 4)\n",
    "model_knn.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model_knn.predict(x_test)\n",
    "accuracy_score(y_pred, y_test)\n",
    "f1_score(y_pred, y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ed9da",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation\n",
    "\n",
    "1. Shuffle the dataset randomly.  \n",
    "2. Split the dataset into *k* groups.  \n",
    "3. For each unique group:  \n",
    "   1. Use the group as a test set (*hold out*).  \n",
    "   2. Use the remaining groups as the training set.  \n",
    "   3. Train a model on the training set and evaluate it on the test set.  \n",
    "   4. Save the evaluation score and discard the model.  \n",
    "4. Summarize the model’s performance using the sample of evaluation scores obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f8da9",
   "metadata": {},
   "source": [
    "### K-Fold\n",
    "\n",
    "Provides training/testing indices to split the data into training and test sets.  \n",
    "Splits the dataset into *k* consecutive folds (without shuffling by default).  \n",
    "Each fold is used once as validation, while the remaining *k - 1* folds form the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b39fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: train=[1393 1394 1395 ... 5569 5570 5571], test=[   0    1    2 ... 1390 1391 1392]\n",
      "1: train=[   0    1    2 ... 5569 5570 5571], test=[1393 1394 1395 ... 2783 2784 2785]\n",
      "2: train=[   0    1    2 ... 5569 5570 5571], test=[2786 2787 2788 ... 4176 4177 4178]\n",
      "3: train=[   0    1    2 ... 4176 4177 4178], test=[4179 4180 4181 ... 5569 5570 5571]\n"
     ]
    }
   ],
   "source": [
    "skf = KFold(n_splits=4)\n",
    "skf.get_n_splits(X, Y)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "    print(f'{i}: train={train_idx}, test={test_idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9997f",
   "metadata": {},
   "source": [
    "### Stratified K-Fold\n",
    "\n",
    "This cross-validation object is a variation of K-Fold that returns stratified folds.  \n",
    "The folds are created by preserving the percentage of samples for each class in *y* in a binary or multiclass classification scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: train=[1227 1229 1252 ... 5569 5570 5571], test=[   0    1    2 ... 1406 1408 1409]\n",
      "1: train=[   0    1    2 ... 5569 5570 5571], test=[1227 1229 1252 ... 2792 2793 2794]\n",
      "2: train=[   0    1    2 ... 5569 5570 5571], test=[2719 2729 2730 ... 4181 4182 4184]\n",
      "3: train=[   0    1    2 ... 4181 4182 4184], test=[4154 4156 4162 ... 5569 5570 5571]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=4)\n",
    "skf.get_n_splits(X, Y)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "    print(f'{i}: train={train_idx}, test={test_idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b0ecb",
   "metadata": {},
   "source": [
    "### T-Test\n",
    "\n",
    "The t-test is a statistical tool used to determine whether there is a significant difference between the means of two groups, or between the mean of a group and a known value.\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=VekJxtk4BYM\n",
    "\n",
    "#### One Sample\n",
    "\n",
    "t = (x̄ - μ) / (s / sqrt(n))\n",
    "\n",
    "x̄ = sample mean  \n",
    "μ = assumed (or population) mean  \n",
    "s = sample standard deviation  \n",
    "n = number of observations (sample size)\n",
    "\n",
    "#### Two Samples\n",
    "\n",
    "t = (x̄₁ - x̄₂) / sqrt((s₁² / n₁) + (s₂² / n₂))\n",
    "\n",
    "x̄₁ = observed mean of the 1st sample  \n",
    "x̄₂ = observed mean of the 2nd sample  \n",
    "s₁ = standard deviation of the 1st sample  \n",
    "s₂ = standard deviation of the 2nd sample  \n",
    "n₁ = size of the 1st sample  \n",
    "n₂ = size of the 2nd sample  \n",
    "\n",
    "#### Bonferroni Correction\n",
    "\n",
    "A statistical method used to control the false positive rate (Type I errors) when performing multiple statistical tests on the same dataset.  \n",
    "If you perform *m* independent tests with a total significance level *α*, the Bonferroni-corrected threshold is:  \n",
    "\n",
    "alpha_corrected = alpha / m  \n",
    "\n",
    "A test is considered significant if:  \n",
    "\n",
    "p_i < alpha_corrected  \n",
    "\n",
    "Alternatively, you can adjust the p-values directly:  \n",
    "\n",
    "p_adjusted = p_i * m  \n",
    "\n",
    "Then, compare the adjusted p-values with the original significance level (alpha).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e295a1",
   "metadata": {},
   "source": [
    "### Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f08ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10)\n",
    "kfold.get_n_splits(X, Y)\n",
    "\n",
    "model_dtree = tree.DecisionTreeClassifier()\n",
    "model_svm = svm.SVC()\n",
    "model_knear = KNeighborsClassifier()\n",
    "\n",
    "acc_dtree, acc_svm, acc_knear = [], [], []\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(X, Y)):\n",
    "    # Variables for the \"blocks\"\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "\n",
    "    # Train the models\n",
    "    model_dtree.fit(X_train, y_train)\n",
    "    model_svm.fit(X_train, y_train)\n",
    "    model_knear.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions for y\n",
    "    y_pred_dtree = model_dtree.predict(X_test)\n",
    "    y_pred_svm   = model_svm.predict(X_test)\n",
    "    y_pred_knear = model_knear.predict(X_test)\n",
    "\n",
    "    # Save the accuracy\n",
    "    acc_dtree.append(accuracy_score(y_test, y_pred_dtree))\n",
    "    acc_svm.append(accuracy_score(y_test, y_pred_svm))\n",
    "    acc_knear.append(accuracy_score(y_test, y_pred_knear))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f84c3",
   "metadata": {},
   "source": [
    "Using Scipy T-test without correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30a44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree vs SVM: t = -3.362, p = 0.008\n",
      "Decision Tree vs KNN: t = 20.798, p = 0.000\n",
      "SVM vs KNN:           t = 26.341, p = 0.000\n"
     ]
    }
   ],
   "source": [
    "# Compare models using T-test\n",
    "t_dtree_svm, p_dtree_svm     = ttest_rel(acc_dtree, acc_svm)\n",
    "t_dtree_knear, p_dtree_knear = ttest_rel(acc_dtree, acc_knear)\n",
    "t_svm_knear, p_svm_knear     = ttest_rel(acc_svm, acc_knear)\n",
    "\n",
    "print(\"Decision Tree vs SVM: t = %.3f, p = %.3f\" % (t_dtree_svm, p_dtree_svm))\n",
    "print(\"Decision Tree vs KNN: t = %.3f, p = %.3f\" % (t_dtree_knear, p_dtree_knear))\n",
    "print(\"SVM vs KNN:           t = %.3f, p = %.3f\" % (t_svm_knear, p_svm_knear))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba6ebf",
   "metadata": {},
   "source": [
    "with correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedfb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree vs SVM: t = -3.362, p = 0.008, significant = True\n",
      "Decision Tree vs KNN: t = 20.798, p = 0.000, significant = True\n",
      "SVM vs KNN: t = 26.341, p = 0.000, significant = True\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "# We have 3 comparisons\n",
    "alpha_bonf = alpha / 3\n",
    "\n",
    "t_dtree_svm, p_dtree_svm     = ttest_rel(acc_dtree, acc_svm)\n",
    "t_dtree_knear, p_dtree_knear = ttest_rel(acc_dtree, acc_knear)\n",
    "t_svm_knear, p_svm_knear     = ttest_rel(acc_svm, acc_knear)\n",
    "\n",
    "print(\"Decision Tree vs SVM: t = %.3f, p = %.3f, significant = %s\" %\n",
    "      (t_dtree_svm, p_dtree_svm, p_dtree_svm < alpha_bonf))\n",
    "print(\"Decision Tree vs KNN: t = %.3f, p = %.3f, significant = %s\" %\n",
    "      (t_dtree_knear, p_dtree_knear, p_dtree_knear < alpha_bonf))\n",
    "print(\"SVM vs KNN: t = %.3f, p = %.3f, significant = %s\" %\n",
    "      (t_svm_knear, p_svm_knear, p_svm_knear < alpha_bonf))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
